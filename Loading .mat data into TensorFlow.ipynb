{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data into Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exploration in how to load external data, especially one that is stored as .mat into TensorFlow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With new TensorFlow API, ```tf.data``` module is the preferred way to load the data in TensorFlow. A ```tf.data.Dataset``` object represents a sequence of elements, each with at least one tensor, such as an image pipeline. A ```tf.data.Iterator``` provides the main way to extract elements from a dataset. The operation returned by ```Iterator.get_next``` yields the next element of a Dataset when executed, and typically acts as the interface between input pipeline code and your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first think about creating a dataset and then worry about the later parts like iterators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can be created from tensors or tensor slices. If we have a numpy array, we can call ```Dataset.from_tensor_slices``` to turn it into a dataset. So it seems fair to first get our .mat file into a numpy array. Luckily, ```scipy.io``` has a function which can help us do that. Let's go ahead and import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scipy, we have function ```scipy.io.loadmat``` which will load the data. It takes various arguments, but we don't need to concern ourselves with those here I think. You can look up the docs if you wish. It will return a dictionary from which we will get our numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Dropbox/Deep Learning Microscope Code/data/thin_smear/raw_data/examples_with_malaria/all_with_malaria_combo_328.mat\"\n",
    "mal_dict = spio.loadmat(path)\n",
    "mal_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there is our data inside the dict. Now, just get it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withMalaria = mal_dict['all_with_malaria_combo']\n",
    "withMalaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withMalaria.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got positive set done. we will repeat the procedure for negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Dropbox/Deep Learning Microscope Code/data/thin_smear/raw_data/examples_without_malaria/all_without_malaria_combo_693.mat\"\n",
    "wmal_dict = spio.loadmat(path)\n",
    "wmal_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutMalaria = wmal_dict['all_without_malaria_combo']\n",
    "withoutMalaria.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have to concatenate these two arrays along their length to get the complete image dataset and we are about a quarter of the way there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageData = np.concatenate((withMalaria, withoutMalaria), axis=3)\n",
    "imageData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Now we gotta make an array for labels. Its better to straight away create one-hot labels since we will need those later anyway. We know that top 328 are postive samples so, we will initialise the top 328 values as ```[1 0]``` and rest as ```[0 1]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros([2,1021])\n",
    "labels[0,0:328] = 1\n",
    "labels[1,328:1021] = 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will just reshape to get the length, which is currently the fourth dimension, as our first. Then we will shuffle the arrays beacuse right now they are in a perfect order which is undesirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageData = imageData.swapaxes(0,3).swapaxes(2,3).swapaxes(1,2)\n",
    "labels = labels.swapaxes(0,1)\n",
    "s = np.arange(labels.shape[0])\n",
    "np.random.shuffle(s)\n",
    "imageData = imageData[s,:,:,:]\n",
    "labels = labels[s,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! We now have the raw materials for getting datasets and iterators using TensorFlow. We will construct out dataset using from_tensor_slices method and use Initializable iterator because we want to create training and testing sets. So One shot iterator is a one that can iterate once through a dataset, you cannot feed any value to it. Initializable is where you can dynamically change calling its initializer operation and passing the new data with feed_dict. There also is a reinitializable and a feedable iterator but I think we don't need them. Feedable is definitely too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make a dataset from tensor One shot. It can iterate once through a dataset, you cannot feed any value to it.\n",
    "EPOCHS = 10\n",
    "# Create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None, 28, 28, 96]), tf.placeholder(tf.float32, shape=[None, 2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "\n",
    "# train-test split\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(imageData, labels, test_size=0.25)\n",
    "\n",
    "# make an iterator to get next batch\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose there is model which is already defined elsewhere and ```pred = model(train_images)``` then the training will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass the second value from iter.get_net() as label\n",
    "loss = tf.losses.mean_squared_error(pred, labels) \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "n_batches = train_images.shape[0] // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={x: train_images, y: train_labels, batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={x: test_images, y: test_labels, batch_size: test_images.shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualize the data to check if it is correct. So we will create a session, run an initialiser and try to plot it using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    idx = 9\n",
    "    sess.run(iter.initializer, feed_dict={x: train_images, y: train_labels, batch_size: 10})\n",
    "    image = sess.run(features)\n",
    "    label = sess.run(labels)\n",
    "    io = image[idx][:,:,1]\n",
    "    print(label[idx])\n",
    "    plt.imshow(io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, it seems legit. So, that is it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
